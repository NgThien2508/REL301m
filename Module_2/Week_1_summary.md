# Tá»•ng káº¿t Tuáº§n 1: K-Armed Bandits vÃ  CÃ¡c KhÃ¡i niá»‡m CÆ¡ báº£n

## 1. Váº¥n Ä‘á» K-Armed Bandit

### 1.1 Äá»‹nh nghÄ©a
- **K-Armed Bandit** lÃ  má»™t bÃ i toÃ¡n trong há»c tÄƒng cÆ°á»ng (Reinforcement Learning) trong Ä‘Ã³:
  - Agent pháº£i chá»n giá»¯a k hÃ nh Ä‘á»™ng khÃ¡c nhau
  - Má»—i hÃ nh Ä‘á»™ng mang láº¡i pháº§n thÆ°á»Ÿng ngáº«u nhiÃªn
  - Má»¥c tiÃªu lÃ  tá»‘i Ä‘a hÃ³a tá»•ng pháº§n thÆ°á»Ÿng

### 1.2 Cáº¥u trÃºc cÆ¡ báº£n
```
Agent â†’ Chá»n 1 trong k hÃ nh Ä‘á»™ng â†’ Nháº­n pháº§n thÆ°á»Ÿng
VÃ­ dá»¥ vá»›i k = 3:
- HÃ nh Ä‘á»™ng 1: Pháº§n thÆ°á»Ÿng trung bÃ¬nh tháº¥p ğŸ˜Ÿ
- HÃ nh Ä‘á»™ng 2: Pháº§n thÆ°á»Ÿng trung bÃ¬nh trung bÃ¬nh ğŸ˜
- HÃ nh Ä‘á»™ng 3: Pháº§n thÆ°á»Ÿng trung bÃ¬nh cao ğŸ˜Š
```

## 2. GiÃ¡ trá»‹ HÃ nh Ä‘á»™ng (Action Values)

### 2.1 Äá»‹nh nghÄ©a toÃ¡n há»c
- **GiÃ¡ trá»‹ thá»±c cá»§a hÃ nh Ä‘á»™ng** (True Action Value):
  
  $$q_*(a) = \mathbb{E}[R_t | A_t = a], \forall a \in \{1,\ldots,k\}$$
  
  Trong Ä‘Ã³:
  - $q_*(a)$: GiÃ¡ trá»‹ thá»±c cá»§a hÃ nh Ä‘á»™ng $a$
  - $\mathbb{E}[R_t]$: Ká»³ vá»ng pháº§n thÆ°á»Ÿng táº¡i thá»i Ä‘iá»ƒm $t$
  - $A_t = a$: Khi chá»n hÃ nh Ä‘á»™ng $a$

### 2.2 Æ¯á»›c lÆ°á»£ng giÃ¡ trá»‹
1. **PhÆ°Æ¡ng phÃ¡p trung bÃ¬nh máº«u (Sample-Average Method)**:
   
   $$Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}} = \frac{\text{Tá»•ng pháº§n thÆ°á»Ÿng khi chá»n }a}{\text{Sá»‘ láº§n chá»n }a}$$

2. **Cáº­p nháº­t gia tÄƒng (Incremental Update)**:
   
   $$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)}[R_t - Q_t(a)]$$
   
   Trong Ä‘Ã³:
   - $Q_t(a)$: Æ¯á»›c lÆ°á»£ng hiá»‡n táº¡i
   - $R_t$: Pháº§n thÆ°á»Ÿng má»›i
   - $N_t(a)$: Sá»‘ láº§n chá»n $a$
   - $[R_t - Q_t(a)]$: Sai sá»‘

## 3. ThÄƒm dÃ² vÃ  Khai thÃ¡c (Exploration vs. Exploitation)

### 3.1 Äá»‹nh nghÄ©a
- **ThÄƒm dÃ² (Exploration)**: 
  - Cáº£i thiá»‡n kiáº¿n thá»©c cho lá»£i Ã­ch dÃ i háº¡n
  - Thá»­ nghiá»‡m cÃ¡c hÃ nh Ä‘á»™ng má»›i

- **Khai thÃ¡c (Exploitation)**:
  - Sá»­ dá»¥ng kiáº¿n thá»©c hiá»‡n cÃ³ cho lá»£i Ã­ch ngáº¯n háº¡n
  - Chá»n hÃ nh Ä‘á»™ng tá»‘t nháº¥t Ä‘Ã£ biáº¿t

### 3.2 PhÆ°Æ¡ng phÃ¡p Îµ-greedy

$$A_t \leftarrow \begin{cases}
    \arg\max_a Q_t(a) & \text{vá»›i xÃ¡c suáº¥t } 1-\varepsilon \\
    a \sim \text{Uniform}(\{a_1,\ldots,a_k\}) & \text{vá»›i xÃ¡c suáº¥t } \varepsilon
\end{cases}$$

- $\varepsilon$: Tá»· lá»‡ thÄƒm dÃ² (thÆ°á»ng 0.1 hoáº·c nhá» hÆ¡n)
- $1-\varepsilon$: Tá»· lá»‡ khai thÃ¡c
- Uniform: Chá»n ngáº«u nhiÃªn Ä‘á»u

## 4. GiÃ¡ trá»‹ Khá»Ÿi táº¡o Láº¡c quan (Optimistic Initial Values)

### 4.1 NguyÃªn lÃ½
- Khá»Ÿi táº¡o $Q_1(a)$ vá»›i giÃ¡ trá»‹ cao hÆ¡n $q_*(a)$
- VÃ­ dá»¥: $Q_1(a) = 2.0$ cho má»i $a$
- Táº¡o Ä‘á»™ng lá»±c thÄƒm dÃ² tá»± nhiÃªn

### 4.2 Æ¯u Ä‘iá»ƒm
- ThÄƒm dÃ² cÃ³ há»‡ thá»‘ng ban Ä‘áº§u
- Tá»± Ä‘á»™ng giáº£m thÄƒm dÃ² theo thá»i gian
- KhÃ´ng cáº§n tham sá»‘ $\varepsilon$

## 5. PhÆ°Æ¡ng phÃ¡p UCB (Upper-Confidence Bound)

### 5.1 CÃ´ng thá»©c

$$A_t = \arg\max_a \left[Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right]$$

Trong Ä‘Ã³:
- $Q_t(a)$: Pháº§n khai thÃ¡c (exploit)
- $c\sqrt{\frac{\ln t}{N_t(a)}}$: Pháº§n thÄƒm dÃ² (explore)
- $c$: Tham sá»‘ kiá»ƒm soÃ¡t má»©c Ä‘á»™ thÄƒm dÃ²

### 5.2 NguyÃªn lÃ½ "Upper-Confidence Bound"
- Tá»± Ä‘á»™ng cÃ¢n báº±ng thÄƒm dÃ² vÃ  khai thÃ¡c
- Æ¯u tiÃªn hÃ nh Ä‘á»™ng Ã­t Ä‘Æ°á»£c thá»­
- Giáº£m thÄƒm dÃ² khi cÃ³ nhiá»u thÃ´ng tin

## 6. Káº¿t luáº­n

K-Armed Bandit lÃ  ná»n táº£ng cho cÃ¡c khÃ¡i niá»‡m quan trá»ng trong há»c tÄƒng cÆ°á»ng:
- CÃ¢n báº±ng thÄƒm dÃ² vÃ  khai thÃ¡c
- Æ¯á»›c lÆ°á»£ng vÃ  cáº­p nháº­t giÃ¡ trá»‹
- Ra quyáº¿t Ä‘á»‹nh trong Ä‘iá»u kiá»‡n khÃ´ng cháº¯c cháº¯n

-------------------------------------------
##### 8-12-2025 at 10PM.
##### Course: Fundamentals of Reinforcement Learning/Module 2.
##### Äá»c tÃ i liá»‡u táº¡i: Week 1 Summary
##### Há»c ná»™i dung tá»« clip: Week 1 Summary
