# Giá trị Khởi tạo Lạc quan (Optimistic Initial Values)

## 1. Lý thuyết cơ bản

### 1.1 Định nghĩa
> Optimistic Initial Values là phương pháp đặt giá trị ước tính ban đầu của các hành động cao hơn giá trị thực tế kỳ vọng của chúng, nhằm khuyến khích agent thăm dò tất cả các hành động trong giai đoạn đầu học tập.

### 1.2 Nguyên lý hoạt động
- Khởi tạo giá trị Q cao hơn giá trị thực tế
- Agent sẽ tự nhiên thăm dò do tính lạc quan
- Giá trị Q dần điều chỉnh về giá trị thực

### 1.3 Công thức cập nhật giá trị Q
$$ Q_{n+1} \leftarrow Q_n + \alpha(R_n - Q_n) $$

Trong đó:
- $Q_{n+1}$: Giá trị ước tính mới sau khi cập nhật
- $Q_n$: Giá trị ước tính hiện tại
- $\alpha$: Tốc độ học (learning rate), $0 < \alpha \leq 1$
- $R_n$: Phần thưởng nhận được ở bước thứ n
- $(R_n - Q_n)$: Sai số giữa phần thưởng thực tế và giá trị ước tính

### 1.4 Ảnh hưởng của tốc độ học α
1. **α lớn** (ví dụ: α = 0.5):
   - Cập nhật nhanh hơn
   - Thích nghi nhanh với thông tin mới
   - Có thể bị ảnh hưởng nhiều bởi nhiễu

2. **α nhỏ** (ví dụ: α = 0.1):
   - Cập nhật chậm hơn
   - Ổn định hơn
   - Ít bị ảnh hưởng bởi nhiễu

## 2. Ví dụ minh họa

### 2.1 Thử nghiệm với ba loại thuốc
1. **Thiết lập ban đầu**:
   - Phần thưởng: 1 nếu điều trị thành công, 0 nếu thất bại
   - Giá trị khởi tạo lạc quan: $Q_1(a) = 2.0$ cho mọi thuốc
   - Tốc độ học: $\alpha = 0.5$

2. **Ví dụ cụ thể**:
   ```
   Bệnh nhân 1:
   - Chọn thuốc P (ngẫu nhiên vì các giá trị bằng nhau)
   - Kết quả: Thành công (R = 1)
   - Cập nhật: Q(P) = 2 + 0.5(1 - 2) = 1.5

   Bệnh nhân 2:
   - Chọn thuốc Y (ngẫu nhiên giữa Y và B có Q = 2)
   - Kết quả: Thất bại (R = 0)
   - Cập nhật: Q(Y) = 2 + 0.5(0 - 2) = 1.0

   Bệnh nhân 3:
   - Chọn thuốc B (Q cao nhất = 2)
   - Kết quả: Thành công (R = 1)
   - Cập nhật: Q(B) = 2 + 0.5(1 - 2) = 1.5
   ```

## 3. Thử nghiệm trên 10-armed Testbed

### 3.1 Giới thiệu về môi trường thử nghiệm
10-armed Testbed là một môi trường thử nghiệm chuẩn trong học tăng cường, bao gồm:
- 10 cánh tay (actions) để lựa chọn
- Mỗi cánh tay có một phân phối phần thưởng riêng
- Phần thưởng tuân theo phân phối chuẩn (normal distribution)
- Giá trị thực $q_*(a)$ của mỗi hành động được chọn từ phân phối chuẩn $\mathcal{N}(0,1)$

### 3.2 Thiết lập thí nghiệm

#### A. Cấu hình thí nghiệm
1. **Phương pháp Realistic, ε-greedy**:
   - Giá trị khởi tạo: $Q_1 = 0$ (thực tế)
   - Xác suất thăm dò: $\varepsilon = 0.1$ (10% thăm dò)
   - Tốc độ học: $\alpha = 0.1$
   - *Đặc điểm*: Bắt đầu với giá trị thực tế, dựa vào ε để thăm dò

2. **Phương pháp Optimistic, greedy**:
   - Giá trị khởi tạo: $Q_1 = 5.0$ (lạc quan)
   - Không có thăm dò ngẫu nhiên: $\varepsilon = 0$
   - Tốc độ học: $\alpha = 0.1$
   - *Đặc điểm*: Dùng giá trị khởi tạo cao để thúc đẩy thăm dò tự nhiên

#### B. Ví dụ thực tế: Hệ thống đề xuất với 2 nhà hàng

Giả sử chúng ta có một ứng dụng đề xuất với chỉ 2 nhà hàng: Nhà hàng A (phở) và Nhà hàng B (cơm).
Điểm đánh giá thực tế trung bình của hai nhà hàng:
- Nhà hàng A: 4.5 sao
- Nhà hàng B: 3.5 sao

1. **Phương pháp Realistic (Thực tế)**:
   ```
   Thiết lập ban đầu:
   Nhà hàng | Giá trị khởi tạo | Xác suất chọn
   ---------|------------------|---------------
   Phở (A)  | 0                | 10% (ε = 0.1)
   Cơm (B)  | 0                | 10% (ε = 0.1)

   Quy tắc chọn:
   - 90% thời gian: Chọn nhà hàng điểm cao nhất
   - 10% thời gian: Chọn ngẫu nhiên một trong hai nhà hàng
   ```

2. **Phương pháp Optimistic (Lạc quan)**:
   ```
   Thiết lập ban đầu:
   Nhà hàng | Giá trị khởi tạo | Xác suất chọn
   ---------|------------------|---------------
   Phở (A)  | 5.0              | 50% (đều nhau)
   Cơm (B)  | 5.0              | 50% (đều nhau)

   Quy tắc chọn:
   - Luôn chọn nhà hàng điểm cao nhất
   - Ban đầu cả hai = 5.0 nên chọn ngẫu nhiên
   ```

#### C. Minh họa quá trình hoạt động (5 lượt đầu)

1. **Phương pháp Realistic**:
   ```
   Lượt 1: 
   - Chọn ngẫu nhiên (vì cả hai = 0)
   - Chọn nhà hàng A, khách cho 4.5 sao
   - Cập nhật: Q(A) = 0 + 0.1(4.5 - 0) = 0.45

   Lượt 2:
   - 90% chọn A (vì 0.45 > 0)
   - Giả sử chọn A, khách cho 4.5 sao
   - Cập nhật: Q(A) = 0.45 + 0.1(4.5 - 0.45) = 0.855

   Lượt 3:
   - May mắn rơi vào 10% thăm dò
   - Chọn B, khách cho 3.5 sao
   - Cập nhật: Q(B) = 0 + 0.1(3.5 - 0) = 0.35

   Lượt 4:
   - 90% chọn A (vì 0.855 > 0.35)
   - Chọn A, khách cho 4.5 sao
   - Cập nhật: Q(A) = 0.855 + 0.1(4.5 - 0.855) = 1.2195

   Lượt 5:
   - 90% chọn A (vì 1.2195 > 0.35)
   - Chọn A, khách cho 4.5 sao
   - Cập nhật: Q(A) = 1.2195 + 0.1(4.5 - 1.2195) ≈ 1.5476
   ```

2. **Phương pháp Optimistic**:
   ```
   Lượt 1:
   - Chọn ngẫu nhiên (vì cả hai = 5.0)
   - Chọn A, khách cho 4.5 sao
   - Cập nhật: Q(A) = 5.0 + 0.1(4.5 - 5.0) = 4.95

   Lượt 2:
   - Chọn B (vì 5.0 > 4.95)
   - Khách cho 3.5 sao
   - Cập nhật: Q(B) = 5.0 + 0.1(3.5 - 5.0) = 4.85

   Lượt 3:
   - Chọn A (vì 4.95 > 4.85)
   - Khách cho 4.5 sao
   - Cập nhật: Q(A) = 4.95 + 0.1(4.5 - 4.95) = 4.905

   Lượt 4:
   - Chọn A (vì 4.905 > 4.85)
   - Khách cho 4.5 sao
   - Cập nhật: Q(A) = 4.905 + 0.1(4.5 - 4.905) ≈ 4.8645

   Lượt 5:
   - Chọn A (vì 4.8645 > 4.85)
   - Khách cho 4.5 sao
   - Cập nhật: Q(A) = 4.8645 + 0.1(4.5 - 4.8645) ≈ 4.82805
   ```

#### D. So sánh hai phương pháp qua ví dụ đơn giản

1. **Phương pháp Realistic**:
   - Bắt đầu từ 0 và tăng dần lên
   - Nhanh chóng tập trung vào nhà hàng A (tốt hơn)
   - Vẫn có 10% cơ hội thử nhà hàng B
   - Giá trị Q tiến dần đến giá trị thực từ dưới lên

2. **Phương pháp Optimistic**:
   - Bắt đầu từ 5.0 và giảm dần xuống
   - Tự nhiên thử cả hai nhà hàng ban đầu
   - Cuối cùng cũng tập trung vào nhà hàng A
   - Giá trị Q tiến dần đến giá trị thực từ trên xuống

3. **Kết luận**:
   - Cả hai phương pháp đều tìm ra nhà hàng tốt nhất (A)
   - Realistic cần tham số ε để đảm bảo thăm dò
   - Optimistic tự động thăm dò nhờ giá trị khởi tạo cao
   - Optimistic thích hợp khi biết chắc giá trị thực không vượt quá 5.0

## 4. Hạn chế và ứng dụng thực tế

### 4.1 Hạn chế của phương pháp
1. **Chỉ thúc đẩy thăm dò giai đoạn đầu**:
   - Sau khi các ước tính hội tụ, agent không còn thăm dò
   - Không phù hợp cho môi trường phi dừng (non-stationary)

2. **Khó xác định giá trị khởi tạo phù hợp**:
   - Cần biết phần thưởng tối đa có thể
   - Giá trị quá lạc quan có thể làm chậm quá trình học

3. **Vấn đề với môi trường phi dừng**:
   - Không phát hiện được thay đổi sau khi đã ổn định
   - Cần kết hợp với các phương pháp thăm dò khác

### 4.2 Ứng dụng thực tế
Mặc dù có những hạn chế, Optimistic Initial Values vẫn là một phương pháp hữu ích và thường được sử dụng:
- Kết hợp với các phương pháp thăm dò khác
- Áp dụng trong giai đoạn đầu học tập
- Sử dụng trong môi trường tương đối ổn định

----------------------------------------------------------------------------------------------------------------------------                                                                                                                                    
##### 5-11-2025 at 5PM.
##### Course: Fundamentals of Reinforcement Learning/Module 2.
##### Đọc tài liệu tại: Exploration vs. Exploitation tradeoff
##### Học nội dung từ clip: Exploration vs. Exploitation tradeoff/Optimistic Initial Values.
