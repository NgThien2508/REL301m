# Giá trị Khởi tạo Lạc quan (Optimistic Initial Values)

## Giới thiệu

Trong học tăng cường (Reinforcement Learning), việc khởi tạo giá trị ban đầu một cách lạc quan là một chiến lược phổ biến để cân bằng giữa thăm dò và khai thác. Phương pháp này dựa trên nguyên tắc "lạc quan trong sự không chắc chắn" (optimistic in the face of uncertainty).

## Nguyên lý hoạt động

### Định nghĩa
> Optimistic Initial Values là phương pháp đặt giá trị ước tính ban đầu của các hành động cao hơn giá trị thực tế kỳ vọng của chúng, nhằm khuyến khích agent thăm dò tất cả các hành động trong giai đoạn đầu học tập.

### Ví dụ:

Xét ví dụ về một bác sĩ thực hiện thử nghiệm với ba loại thuốc (P, Y, B):

1. **Thiết lập ban đầu**:
   - Phần thưởng: 1 nếu điều trị thành công, 0 nếu thất bại
   - Giá trị khởi tạo lạc quan: $Q_1(a) = 2.0$ cho mọi thuốc
   - Tốc độ học: $\alpha = 0.5$

2. **Quy trình cập nhật**:
   $$ Q_{n+1} \leftarrow Q_n + \alpha(R_n - Q_n) $$

3. **Ví dụ cụ thể**:
   ```
   Bệnh nhân 1:
   - Chọn thuốc P (ngẫu nhiên vì các giá trị bằng nhau)
   - Kết quả: Thành công (R = 1)
   - Cập nhật: Q(P) = 2 + 0.5(1 - 2) = 1.5

   Bệnh nhân 2:
   - Chọn thuốc Y (ngẫu nhiên giữa Y và B có Q = 2)
   - Kết quả: Thất bại (R = 0)
   - Cập nhật: Q(Y) = 2 + 0.5(0 - 2) = 1.0

   Bệnh nhân 3:
   - Chọn thuốc B (Q cao nhất = 2)
   - Kết quả: Thành công (R = 1)
   - Cập nhật: Q(B) = 2 + 0.5(1 - 2) = 1.5
   ```

## Thử nghiệm trên 10-armed Testbed

### Thiết lập thí nghiệm
- So sánh hai phương pháp:
  1. **Realistic, ε-greedy**: $Q_1 = 0$, $\varepsilon = 0.1$, $\alpha = 0.1$
  2. **Optimistic, greedy**: $Q_1 = 5.0$, $\varepsilon = 0$, $\alpha = 0.1$

### Kết quả thực nghiệm
- **Giai đoạn đầu**: Agent lạc quan có hiệu suất thấp hơn do thăm dò nhiều
- **Giai đoạn sau**: Độ lạc quan giảm dần, agent học được các giá trị thực
- **Hiệu suất dài hạn**: Agent lạc quan đạt được hiệu suất tốt hơn

## Hạn chế của phương pháp

1. **Chỉ thúc đẩy thăm dò giai đoạn đầu**:
   - Sau khi các ước tính hội tụ, agent không còn thăm dò
   - Không phù hợp cho môi trường phi dừng (non-stationary)

2. **Khó xác định giá trị khởi tạo phù hợp**:
   - Cần biết phần thưởng tối đa có thể
   - Giá trị quá lạc quan có thể làm chậm quá trình học

3. **Vấn đề với môi trường phi dừng**:
   - Không phát hiện được thay đổi sau khi đã ổn định
   - Cần kết hợp với các phương pháp thăm dò khác

## Ứng dụng thực tế

Mặc dù có những hạn chế, Optimistic Initial Values vẫn là một phương pháp hữu ích và thường được sử dụng:
- Kết hợp với các phương pháp thăm dò khác
- Áp dụng trong giai đoạn đầu học tập
- Sử dụng trong môi trường tương đối ổn định

----------------------------------------------------------------------------------------------------------------------------                                                                                                                                    
##### 8-12-2025 at 5PM.
##### Course: Fundamentals of Reinforcement Learning/Module 2.
##### Đọc tài liệu tại: Exploration vs. Exploitation tradeoff
##### Học nội dung từ clip: Exploration vs. Exploitation tradeoff/Optimistic Initial Values.
