# Giá trị Khởi tạo Lạc quan (Optimistic Initial Values)

## 1. Lý thuyết cơ bản

### 1.1 Định nghĩa
> Optimistic Initial Values là phương pháp đặt giá trị ước tính ban đầu của các hành động cao hơn giá trị thực tế kỳ vọng của chúng, nhằm khuyến khích agent thăm dò tất cả các hành động trong giai đoạn đầu học tập.

### 1.2 Nguyên lý hoạt động
- Khởi tạo giá trị Q cao hơn giá trị thực tế
- Agent sẽ tự nhiên thăm dò do tính lạc quan
- Giá trị Q dần điều chỉnh về giá trị thực

### 1.3 Công thức cập nhật giá trị Q
$$ Q_{n+1} \leftarrow Q_n + \alpha(R_n - Q_n) $$

Trong đó:
- $Q_{n+1}$: Giá trị ước tính mới sau khi cập nhật
- $Q_n$: Giá trị ước tính hiện tại
- $\alpha$: Tốc độ học (learning rate), $0 < \alpha \leq 1$
- $R_n$: Phần thưởng nhận được ở bước thứ n
- $(R_n - Q_n)$: Sai số giữa phần thưởng thực tế và giá trị ước tính

### 1.4 Ảnh hưởng của tốc độ học α
1. **α lớn** (ví dụ: α = 0.5):
   - Cập nhật nhanh hơn
   - Thích nghi nhanh với thông tin mới
   - Có thể bị ảnh hưởng nhiều bởi nhiễu

2. **α nhỏ** (ví dụ: α = 0.1):
   - Cập nhật chậm hơn
   - Ổn định hơn
   - Ít bị ảnh hưởng bởi nhiễu

## 2. Ví dụ minh họa

### 2.1 Thử nghiệm với ba loại thuốc
1. **Thiết lập ban đầu**:
   - Phần thưởng: 1 nếu điều trị thành công, 0 nếu thất bại
   - Giá trị khởi tạo lạc quan: $Q_1(a) = 2.0$ cho mọi thuốc
   - Tốc độ học: $\alpha = 0.5$

2. **Ví dụ cụ thể**:
   ```
   Bệnh nhân 1:
   - Chọn thuốc P (ngẫu nhiên vì các giá trị bằng nhau)
   - Kết quả: Thành công (R = 1)
   - Cập nhật: Q(P) = 2 + 0.5(1 - 2) = 1.5

   Bệnh nhân 2:
   - Chọn thuốc Y (ngẫu nhiên giữa Y và B có Q = 2)
   - Kết quả: Thất bại (R = 0)
   - Cập nhật: Q(Y) = 2 + 0.5(0 - 2) = 1.0

   Bệnh nhân 3:
   - Chọn thuốc B (Q cao nhất = 2)
   - Kết quả: Thành công (R = 1)
   - Cập nhật: Q(B) = 2 + 0.5(1 - 2) = 1.5
   ```

## 3. Thử nghiệm trên 10-armed Testbed

### 3.1 Giới thiệu về môi trường thử nghiệm
10-armed Testbed là một môi trường thử nghiệm chuẩn trong học tăng cường, bao gồm:
- 10 cánh tay (actions) để lựa chọn
- Mỗi cánh tay có một phân phối phần thưởng riêng
- Phần thưởng tuân theo phân phối chuẩn (normal distribution)
- Giá trị thực $q_*(a)$ của mỗi hành động được chọn từ phân phối chuẩn $\mathcal{N}(0,1)$

### 3.2 Thiết lập thí nghiệm
1. **Phương pháp Realistic, ε-greedy**:
   - Giá trị khởi tạo: $Q_1 = 0$ (thực tế)
   - Xác suất thăm dò: $\varepsilon = 0.1$ (10% thăm dò)
   - Tốc độ học: $\alpha = 0.1$
   - *Đặc điểm*: Bắt đầu với giá trị thực tế, dựa vào ε để thăm dò

2. **Phương pháp Optimistic, greedy**:
   - Giá trị khởi tạo: $Q_1 = 5.0$ (lạc quan)
   - Không có thăm dò ngẫu nhiên: $\varepsilon = 0$
   - Tốc độ học: $\alpha = 0.1$
   - *Đặc điểm*: Dùng giá trị khởi tạo cao để thúc đẩy thăm dò tự nhiên

### 3.3 Diễn biến quá trình học
1. **Giai đoạn đầu (0-200 steps)**:
   - *Agent lạc quan*:
     + Bắt đầu với $Q_1 = 5.0$ cho mọi hành động
     + Thử nhiều hành động khác nhau vì tất cả đều có giá trị cao
     + Hiệu suất thấp hơn do dành nhiều thời gian thăm dò
   - *Agent thực tế*:
     + Bắt đầu với $Q_1 = 0$ và dựa vào ε để thăm dò
     + Nhanh chóng tìm được một hành động tốt để khai thác
     + Hiệu suất ban đầu tốt hơn do tập trung khai thác sớm

2. **Giai đoạn giữa (200-600 steps)**:
   - *Agent lạc quan*:
     + Giá trị Q dần giảm về gần giá trị thực
     + Bắt đầu phân biệt được hành động tốt/xấu
     + Hiệu suất tăng nhanh khi đã khám phá đủ
   - *Agent thực tế*:
     + Tiếp tục cân bằng giữa thăm dò và khai thác
     + Hiệu suất tăng chậm nhưng ổn định
     + Vẫn có 10% thời gian thăm dò ngẫu nhiên

3. **Giai đoạn cuối (600-1000 steps)**:
   - *Agent lạc quan*:
     + Đã học được giá trị thực của các hành động
     + Tập trung khai thác hành động tốt nhất
     + Đạt hiệu suất cao và ổn định (>80%)
   - *Agent thực tế*:
     + Vẫn duy trì mức thăm dò 10%
     + Hiệu suất thấp hơn do thăm dò liên tục
     + Đạt hiệu suất khoảng 75%

### 3.4 Phân tích kết quả
1. **Ưu điểm của phương pháp lạc quan**:
   - Thăm dò tự nhiên không cần tham số ε
   - Hiệu suất cuối cùng tốt hơn
   - Thích hợp cho môi trường tĩnh

2. **Nhược điểm của phương pháp lạc quan**:
   - Khởi đầu chậm do thăm dò nhiều
   - Cần thời gian để "phá vỡ" tính lạc quan
   - Không thích hợp cho môi trường động

3. **So sánh với ε-greedy**:
   | Tiêu chí | Optimistic-Greedy | ε-greedy |
   |----------|-------------------|-----------|
   | Khởi đầu | Chậm | Nhanh |
   | Thăm dò | Tự nhiên, giảm dần | Cố định |
   | Hiệu suất cuối | Cao hơn (~85%) | Thấp hơn (~75%) |
   | Ổn định | Rất ổn định | Dao động nhẹ |

## 4. Hạn chế và ứng dụng thực tế

### 4.1 Hạn chế của phương pháp
1. **Chỉ thúc đẩy thăm dò giai đoạn đầu**:
   - Sau khi các ước tính hội tụ, agent không còn thăm dò
   - Không phù hợp cho môi trường phi dừng (non-stationary)

2. **Khó xác định giá trị khởi tạo phù hợp**:
   - Cần biết phần thưởng tối đa có thể
   - Giá trị quá lạc quan có thể làm chậm quá trình học

3. **Vấn đề với môi trường phi dừng**:
   - Không phát hiện được thay đổi sau khi đã ổn định
   - Cần kết hợp với các phương pháp thăm dò khác

### 4.2 Ứng dụng thực tế
Mặc dù có những hạn chế, Optimistic Initial Values vẫn là một phương pháp hữu ích và thường được sử dụng:
- Kết hợp với các phương pháp thăm dò khác
- Áp dụng trong giai đoạn đầu học tập
- Sử dụng trong môi trường tương đối ổn định

----------------------------------------------------------------------------------------------------------------------------                                                                                                                                    
##### 8-12-2025 at 5PM.
##### Course: Fundamentals of Reinforcement Learning/Module 2.
##### Đọc tài liệu tại: Exploration vs. Exploitation tradeoff
##### Học nội dung từ clip: Exploration vs. Exploitation tradeoff/Optimistic Initial Values.
