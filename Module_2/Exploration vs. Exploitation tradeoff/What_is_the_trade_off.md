# Sự đánh đổi giữa Thăm dò và Khai thác (Exploration-Exploitation Trade-off)

## Giới thiệu

Trong học tăng cường (Reinforcement Learning), một trong những thách thức quan trọng nhất là cân bằng giữa **thăm dò** (exploration) và **khai thác** (exploitation). Hãy xem xét một ví dụ thực tế để hiểu rõ hơn về sự đánh đổi này.

### Ví dụ thực tế: Chọn món ăn tại nhà hàng

Tưởng tượng bạn đang đi ăn với bạn bè tối nay. Khi đến nhà hàng, bạn phải quyết định gọi món gì. Bạn đã đến đây vài lần trước và luôn gọi cùng một món. Bạn biết rằng mình sẽ khá hài lòng nếu gọi lại món đó. Tuy nhiên, nhiều món khác trông cũng rất hấp dẫn.

**Câu hỏi đặt ra:** Làm thế nào để quyết định khi nào nên gọi lại món ăn quen thuộc (khai thác), và khi nào nên thử món mới (thăm dò)?

## Thăm dò (Exploration)

### Định nghĩa:
> Thăm dò là quá trình agent khám phá các hành động mới để thu thập thông tin về môi trường và cải thiện ước tính về giá trị của các hành động, ngay cả khi điều này có nghĩa là từ bỏ phần thưởng ngắn hạn để đổi lấy khả năng tìm được chiến lược tốt hơn trong tương lai.

### Đặc điểm của thăm dò:
- Mục tiêu: Cải thiện kiến thức về các hành động
- Lợi ích: Mang lại lợi ích dài hạn
- Phương pháp: Thử nghiệm các hành động mới
- Rủi ro: Có thể nhận được phần thưởng thấp trong ngắn hạn

## Khai thác (Exploitation)

### Định nghĩa:
> Khai thác là quá trình agent sử dụng kiến thức hiện có để chọn hành động được cho là tốt nhất dựa trên các ước tính giá trị hiện tại, nhằm tối đa hóa phần thưởng tức thời.

### Đặc điểm của khai thác:
- Mục tiêu: Tối đa hóa phần thưởng ngắn hạn
- Lợi ích: Đảm bảo phần thưởng ổn định
- Phương pháp: Chọn hành động có giá trị ước tính cao nhất
- Rủi ro: Có thể bỏ lỡ các hành động tốt hơn

## Phương pháp Epsilon-Greedy

### Định nghĩa:
> Epsilon-Greedy là một chiến lược đơn giản để cân bằng giữa thăm dò và khai thác, trong đó agent thực hiện hành động tham lam (greedy) với xác suất 1-ε và chọn một hành động ngẫu nhiên với xác suất ε.

### Công thức toán học và giải thích:

Hành động được chọn tại thời điểm t (At) được xác định bởi:

$$ A_t = \begin{cases} 
\arg\max_a Q_t(a) & \text{với xác suất } 1-\varepsilon \text{ (khai thác)} \\
a \sim \text{Uniform}(a_1,\ldots,a_k) & \text{với xác suất } \varepsilon \text{ (thăm dò)}
\end{cases} $$

Trong đó:
- $A_t$: Hành động được chọn tại thời điểm t
- $Q_t(a)$: Giá trị ước tính của hành động a tại thời điểm t
- $\arg\max_a$: Toán tử chọn hành động a có giá trị $Q_t(a)$ lớn nhất
- $\text{Uniform}(a_1,\ldots,a_k)$: Phân phối đều trên tất cả k hành động có thể
- $\varepsilon$: Tham số kiểm soát tỷ lệ giữa thăm dò và khai thác $(0 \leq \varepsilon \leq 1)$

### Công thức cập nhật giá trị Q:

$$ Q_{t+1}(a) = Q_t(a) + \alpha[R_t - Q_t(a)] $$

Trong đó:
- $Q_{t+1}(a)$: Giá trị mới của hành động a
- $Q_t(a)$: Giá trị hiện tại của hành động a
- $\alpha$: Tốc độ học $(0 < \alpha \leq 1)$
- $R_t$: Phần thưởng nhận được tại thời điểm t

### Ví dụ cụ thể với bài toán nhà hàng:

Giả sử có 3 nhà hàng A, B, C với các giá trị Q hiện tại như sau:
```
Q_t(A) = 4.5  # Điểm đánh giá trung bình của nhà hàng A
Q_t(B) = 3.8  # Điểm đánh giá trung bình của nhà hàng B
Q_t(C) = 0.0  # Chưa từng thử nhà hàng C
```

Với $\varepsilon = 0.1$ (10% cơ hội thăm dò):

1. **Bước 1**: Tạo số ngẫu nhiên r trong khoảng [0,1]
2. **Bước 2**: So sánh r với $\varepsilon$
   - Nếu r < 0.1 (xác suất 10%): 
     * THĂM DÒ: Chọn ngẫu nhiên 1 trong 3 nhà hàng A, B, C
     * VD: Có thể chọn C dù chưa có đánh giá
   - Nếu r ≥ 0.1 (xác suất 90%):
     * KHAI THÁC: Chọn nhà hàng có điểm cao nhất
     * Trong trường hợp này sẽ chọn A (Q_t(A) = 4.5 là cao nhất)

### Ví dụ về cập nhật giá trị Q:

Giả sử chọn thăm dò nhà hàng C và nhận được đánh giá 4.2:

$$ Q_{t+1}(C) = Q_t(C) + \alpha[R_t - Q_t(C)] $$
$$ = 0 + 0.1[4.2 - 0] $$
$$ = 0.42 $$

Trong đó:
- $\alpha = 0.1$ là tốc độ học
- $R_t = 4.2$ là phần thưởng thực tế nhận được
- $Q_t(C) = 0$ là giá trị ước tính ban đầu

### Ví dụ thực nghiệm: 10-armed Testbed

Để đánh giá hiệu quả của phương pháp Epsilon-Greedy, các thí nghiệm được thực hiện trên môi trường 10-armed Testbed với các giá trị $\varepsilon$ khác nhau:

- $\varepsilon = 0$ (pure greedy): Không có thăm dò, chỉ khai thác
- $\varepsilon = 0.01$: Thăm dò 1% thời gian
- $\varepsilon = 0.1$: Thăm dò 10% thời gian

Kết quả cho thấy:
- $\varepsilon = 0.1$ đạt được phần thưởng cao hơn trong giai đoạn đầu
- $\varepsilon = 0.01$ cải thiện liên tục và đạt hiệu suất tốt trong dài hạn
- $\varepsilon = 0$ (pure greedy) có thể bị mắc kẹt ở giải pháp dưới tối ưu

## Kết luận

### Tổng quan về Trade-off
Sự đánh đổi giữa thăm dò và khai thác là một vấn đề cốt lõi trong học tăng cường. Không có giải pháp hoàn hảo cho mọi tình huống, nhưng phương pháp Epsilon-Greedy cung cấp một cách tiếp cận đơn giản và hiệu quả để cân bằng giữa hai mục tiêu này. Việc lựa chọn giá trị $\varepsilon$ phù hợp phụ thuộc vào đặc điểm cụ thể của bài toán và mục tiêu của agent.

### Áp dụng thực tế.
   - Quảng cáo trực tuyến: Cân bằng giữa hiển thị quảng cáo mới và đã biết hiệu quả
   - Hệ thống gợi ý: Giới thiệu cả nội dung mới và nội dung phổ biến
   - Robot học tập: Khám phá môi trường mới trong khi duy trì hiệu suất

----------------------------------------------------------------------------------------------------------------------------                                                                                                                                    
##### 8-12-2025 at 10AM.
##### Course: Fundamentals of Reinforcement Learning/Module 2.
##### Đọc tài liệu tại: Exploration vs. Exploitation tradeoff
##### Học nội dung từ clip: Exploration vs. Exploitation tradeoff/ What is the trade off?.