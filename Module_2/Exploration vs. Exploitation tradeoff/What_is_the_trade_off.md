# Sự đánh đổi giữa Thăm dò và Khai thác (Exploration-Exploitation Trade-off)

## Giới thiệu

Trong học tăng cường, một trong những thách thức quan trọng nhất là cân bằng giữa **thăm dò** (exploration) và **khai thác** (exploitation). Hãy xem xét một ví dụ thực tế để hiểu rõ hơn về sự đánh đổi này.

### Ví dụ thực tế: Chọn món ăn tại nhà hàng

Tưởng tượng bạn đang đi ăn với bạn bè tối nay. Khi đến nhà hàng, bạn phải quyết định gọi món gì. Bạn đã đến đây vài lần trước và luôn gọi cùng một món. Bạn biết rằng mình sẽ khá hài lòng nếu gọi lại món đó. Tuy nhiên, nhiều món khác trông cũng rất hấp dẫn.

**Câu hỏi đặt ra:** Làm thế nào để quyết định khi nào nên gọi lại món ăn quen thuộc (khai thác), và khi nào nên thử món mới (thăm dò)?

## Thăm dò (Exploration)

Thăm dò cho phép agent cải thiện kiến thức về mỗi hành động, hướng đến lợi ích dài hạn. Bằng cách cải thiện độ chính xác của các giá trị hành động ước tính, agent có thể đưa ra quyết định sáng suốt hơn trong tương lai.

### Đặc điểm của thăm dò:
- Mục tiêu: Cải thiện kiến thức về các hành động
- Lợi ích: Mang lại lợi ích dài hạn
- Phương pháp: Thử nghiệm các hành động mới
- Rủi ro: Có thể nhận được phần thưởng thấp trong ngắn hạn

## Khai thác (Exploitation)

Khai thác tận dụng các giá trị ước tính hiện tại của agent. Nó chọn hành động tham lam (greedy action) để cố gắng đạt được phần thưởng cao nhất. Tuy nhiên, việc tham lam dựa trên các giá trị ước tính có thể không thực sự mang lại phần thưởng tốt nhất.

### Đặc điểm của khai thác:
- Mục tiêu: Tối đa hóa phần thưởng ngắn hạn
- Lợi ích: Đảm bảo phần thưởng ổn định
- Phương pháp: Chọn hành động có giá trị ước tính cao nhất
- Rủi ro: Có thể bỏ lỡ các hành động tốt hơn

## Phương pháp Epsilon-Greedy

Epsilon-Greedy là một phương pháp đơn giản để cân bằng giữa thăm dò và khai thác. Phương pháp này hoạt động như sau:

- Với xác suất ε (epsilon): Chọn một hành động ngẫu nhiên (thăm dò)
- Với xác suất 1-ε: Chọn hành động tham lam (khai thác)

### Công thức toán học và giải thích:

```
A_t = {
    argmax_a Q_t(a)     với xác suất 1-ε    # Khai thác: chọn hành động có giá trị Q cao nhất
    a ~ Uniform(a_1...a_k)   với xác suất ε  # Thăm dò: chọn ngẫu nhiên từ tất cả hành động
}
```

Trong đó:
- A_t: Hành động được chọn tại thời điểm t
- Q_t(a): Giá trị ước tính của hành động a tại thời điểm t
- argmax_a: Chọn hành động a có giá trị Q_t(a) lớn nhất
- Uniform(a_1...a_k): Phân phối đều trên tất cả k hành động có thể
- ε: Tham số kiểm soát tỷ lệ giữa thăm dò và khai thác (0 ≤ ε ≤ 1)

### Ví dụ cụ thể với bài toán nhà hàng:

Giả sử có 3 nhà hàng A, B, C với các giá trị Q hiện tại như sau:
```
Q_t(A) = 4.5  # Điểm đánh giá trung bình của nhà hàng A
Q_t(B) = 3.8  # Điểm đánh giá trung bình của nhà hàng B
Q_t(C) = 0.0  # Chưa từng thử nhà hàng C
```

Với ε = 0.1 (10% cơ hội thăm dò):

1. **Bước 1**: Tạo số ngẫu nhiên r trong khoảng [0,1]
2. **Bước 2**: So sánh r với ε
   - Nếu r < 0.1 (xác suất 10%): 
     * THĂM DÒ: Chọn ngẫu nhiên 1 trong 3 nhà hàng A, B, C
     * VD: Có thể chọn C dù chưa có đánh giá
   - Nếu r ≥ 0.1 (xác suất 90%):
     * KHAI THÁC: Chọn nhà hàng có điểm cao nhất
     * Trong trường hợp này sẽ chọn A (Q_t(A) = 4.5 là cao nhất)

### Ví dụ về cập nhật giá trị Q:

Giả sử chọn thăm dò nhà hàng C và nhận được đánh giá 4.2:
```
Q_t+1(C) = Q_t(C) + α[R_t - Q_t(C)]
         = 0 + 0.1[4.2 - 0]
         = 0.42
```
Trong đó:
- α = 0.1 là tốc độ học
- R_t = 4.2 là phần thưởng thực tế nhận được
- Q_t(C) = 0 là giá trị ước tính ban đầu

Qua thời gian, giá trị Q của các hành động sẽ hội tụ về giá trị thực của chúng, giúp agent có thể đưa ra quyết định tốt hơn.

### Ví dụ thực nghiệm: 10-armed Testbed

Để đánh giá hiệu quả của phương pháp Epsilon-Greedy, các thí nghiệm được thực hiện trên môi trường 10-armed Testbed với các giá trị ε khác nhau:

- ε = 0 (pure greedy): Không có thăm dò, chỉ khai thác
- ε = 0.01: Thăm dò 1% thời gian
- ε = 0.1: Thăm dò 10% thời gian

Kết quả cho thấy:
- ε = 0.1 đạt được phần thưởng cao hơn trong giai đoạn đầu
- ε = 0.01 cải thiện liên tục và đạt hiệu suất tốt trong dài hạn
- ε = 0 (pure greedy) có thể bị mắc kẹt ở giải pháp dưới tối ưu

## Kết luận

Sự đánh đổi giữa thăm dò và khai thác là một vấn đề cốt lõi trong học tăng cường. Không có giải pháp hoàn hảo cho mọi tình huống, nhưng phương pháp Epsilon-Greedy cung cấp một cách tiếp cận đơn giản và hiệu quả để cân bằng giữa hai mục tiêu này. Việc lựa chọn giá trị ε phù hợp phụ thuộc vào đặc điểm cụ thể của bài toán và mục tiêu của agent.

   ----------------------------------------------------------------------------------------------------------------------------                                                                                                                                    
  ##### 8-12-2025 at 10AM.
  ##### Course: Fundamentals of Reinforcement Learning/Module 2.
  ##### Đọc tài liệu tại: Exploration vs. Exploitation tradeoff
  ##### Học nội dung từ clip: Exploration vs. Exploitation tradeoff/ What is the trade off?.